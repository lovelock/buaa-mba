<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>最小二乘法 on MBA学习记录</title><link>https://buaa-mba.pages.dev/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/</link><description>Recent content in 最小二乘法 on MBA学习记录</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 22 Sep 2024 21:19:15 +0800</lastBuildDate><atom:link href="https://buaa-mba.pages.dev/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/index.xml" rel="self" type="application/rss+xml"/><item><title>最小二乘法详解与拉格朗日求解过程</title><link>https://buaa-mba.pages.dev/p/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E8%AF%A6%E8%A7%A3%E4%B8%8E%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%B1%82%E8%A7%A3%E8%BF%87%E7%A8%8B/</link><pubDate>Sun, 22 Sep 2024 21:19:15 +0800</pubDate><guid>https://buaa-mba.pages.dev/p/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E8%AF%A6%E8%A7%A3%E4%B8%8E%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%B1%82%E8%A7%A3%E8%BF%87%E7%A8%8B/</guid><description>&lt;h2 id="引言">引言
&lt;/h2>&lt;p>最小二乘法是一种广泛应用于数据拟合和统计建模的数学优化技术。它的基本思想是通过最小化误差的平方和，找到最佳的参数估计，以使得模型最符合观测数据。本文将详细介绍最小二乘法的原理、应用及其通过拉格朗日乘数法求解的过程。&lt;/p>
&lt;h2 id="一最小二乘法的基本原理">一、最小二乘法的基本原理
&lt;/h2>&lt;p>在最小二乘法中，我们希望通过建立一个数学模型来描述自变量与因变量之间的关系。假设我们有 n 个观测数据点，数据点的形式为 \((x_i, y_i)\)（其中 \(i = 1, 2, \ldots, n\)），我们希望找到一个函数 \(f(x)\) 来拟合这些数据。&lt;/p>
&lt;h3 id="1-目标函数">1. 目标函数
&lt;/h3>&lt;p>最小二乘法的目标是最小化残差平方和（RSS），即：&lt;/p>
\[
RSS = \sum_{i=1}^{n} (y_i - f(x_i))^2
\]&lt;p>通常情况下，我们假设 \(f(x)\) 是线性函数，形式为：&lt;/p>
\[
f(x) = \beta_0 + \beta_1 x
\]&lt;p>其中，\(\beta_0\) 和 \(\beta_1\) 是我们需要估计的参数。&lt;/p>
&lt;blockquote>
&lt;p>上面的RSS全称Residual Sum of Squares，和SSE(Sum of Squares for Error)是同一个概念，在某些情况下SSE可能会用在更广泛的上下文中，包括多种误差度量，而RSS特指基于残差的平方和。&lt;/p>
&lt;/blockquote>
&lt;h3 id="2-残差">2. 残差
&lt;/h3>&lt;p>残差 \(e_i\) 定义为观测值与预测值之间的差异：&lt;/p>
\[
e_i = y_i - (\beta_0 + \beta_1 x_i)
\]&lt;p>因此，残差平方和可以表示为：&lt;/p>
\[
RSS(\beta_0, \beta_1) = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2
\]&lt;h2 id="二拉格朗日乘数法求解">二、拉格朗日乘数法求解
&lt;/h2>&lt;p>为了找到使得残差平方和最小的参数 \(\beta_0\) 和 \(\beta_1\)，我们可以使用拉格朗日乘数法。这种方法在约束优化中非常有效。&lt;/p>
&lt;h3 id="1-构建拉格朗日函数">1. 构建拉格朗日函数
&lt;/h3>&lt;p>在最小二乘法中，我们没有约束条件，因此我们可以直接对目标函数进行求导。我们定义拉格朗日函数 \(L\) 为：&lt;/p>
\[
L(\beta_0, \beta_1) = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2
\]&lt;h3 id="2-求导数并设置为零">2. 求导数并设置为零
&lt;/h3>&lt;p>为了找到最优解，我们需要对 \(L\) 分别对 \(\beta_0\) 和 \(\beta_1\) 进行求导，并将导数设置为零。&lt;/p>
&lt;h4 id="a-对-beta_0-的导数">a. 对 \(\beta_0\) 的导数
&lt;/h4>\[
\frac{\partial L}{\partial \beta_0} = -2 \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))
\]&lt;p>设置为零：&lt;/p>
\[
\sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i)) = 0
\]&lt;h4 id="b-对-beta_1-的导数">b. 对 \(\beta_1\) 的导数
&lt;/h4>\[
\frac{\partial L}{\partial \beta_1} = -2 \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i)) x_i
\]&lt;p>设置为零：&lt;/p>
\[
\sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i)) x_i = 0
\]&lt;h3 id="3-联立方程求解">3. 联立方程求解
&lt;/h3>&lt;p>通过上述两个方程，我们可以得到一组关于 \(\beta_0\) 和 \(\beta_1\) 的线性方程组。对这两个方程进行求解可以得到参数的最优估计。&lt;/p>
&lt;h4 id="a-方程一对-beta_0-的方程">a. 方程一（对 \(\beta_0\) 的方程）：
&lt;/h4>\[
n\bar{y} = n\beta_0 + \beta_1 \sum_{i=1}^{n} x_i
\]&lt;h4 id="b-方程二对-beta_1-的方程">b. 方程二（对 \(\beta_1\) 的方程）：
&lt;/h4>\[
\sum_{i=1}^{n} y_i x_i = \beta_0 \sum_{i=1}^{n} x_i + \beta_1 \sum_{i=1}^{n} x_i^2
\]&lt;p>这里，\(\bar{y}\) 是 \(y_i\) 的平均值。&lt;/p>
&lt;h3 id="4-解的最终表达式">4. 解的最终表达式
&lt;/h3>&lt;p>通过解这两个方程，我们可以得到 \(\beta_0\) 和 \(\beta_1\) 的具体值：&lt;/p>
\[
\beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\]\[
\beta_0 = \bar{y} - \beta_1 \bar{x}
\]&lt;p>其中，\(\bar{x}\) 是 \(x_i\) 的平均值。&lt;/p>
&lt;h2 id="三总结">三、总结
&lt;/h2>&lt;p>最小二乘法是一种有效的数据拟合方法，广泛应用于回归分析中。通过最小化残差平方和，可以找到最佳的参数估计。在具体求解过程中，拉格朗日乘数法为我们提供了一种系统化的方法，使得求解过程清晰且高效。理解最小二乘法的原理和求解方法，对于数据分析和建模具有重要的理论和实践价值。&lt;/p></description></item></channel></rss>