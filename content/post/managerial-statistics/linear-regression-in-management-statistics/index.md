---
title: "管理统计中的线性回归"
description: 
date: 2024-10-12T12:33:37+08:00
image: 
math: true
license: 
hidden: false
comments: true
categories: ["管理统计"]
tags: ["线性回归", "R语言", "Julia", "Excel", "SPSS"]
---

线性回归是管理统计这门课目前为止相对体系化的一个章节，本文希望能比较完整地介绍其中的知识点，以帮助笔者和读者形成较为完整的知识图谱。

## 相关系数（The Correlation Coefficient）

在介绍具体的求解方法之前，先来看一下相关系数。

相关系数是用来衡量两个变量之间关系强度和方向的统计量，最常见的有以下3个。

### 皮尔逊相关系数（Pearson correlation coefficient）

#### 定义

衡量两个变量之间的线性关系强度和方向

#### 计算方法

$$
r(x, y) = \frac{\sum^n_{i=1}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum^n_{i=1}(x_i-\bar{x})^2}\sqrt{\sum^n_{i=1}(y_i-\bar{y})^2}}
$$

#### 取值范围

1. $-1 \leq r \leq 1$
2. $0 < r \leq 1$ 正线性相关
3. $-1\leq r < 0$ 负线性相关
4. $r=0$ 线性无关
5. $r=1$ 完全正线性相关
6. $r=-1$ 完全负线性相关

#### 适用场景

适合于数据满足正态分布和线性关系的情况。

### 斯皮尔曼秩相关系数（Spearman's rank correlation coefficient）

#### 定义

衡量两个变量之间的单调关系，基于变量的排名。这也是为什么名字就叫“秩”相关系数的原因。

#### 计算方法

$$
\rho(u,v) = \frac{\sum^n_{i=1}(u_i - \bar{u})(v_i - \bar{v})}{\sqrt{\sum^n_{i=1}(u_i-\bar{u})^2}\sqrt{\sum^n_{i=1}(v_i - \bar{v})^2}}
$$

注意：公式中使用的变量是原始数据排序之后得到的序号值，也就是秩，而不是原始的数据，抛开这个差异，它和Pearson相关系数的公式是一样的。

#### 取值范围

从 -1 到 +1。

- +1: 完全正相关
- -1: 完全负相关
- 0: 无单调关系

注意： Spearman秩相关系数解决了Pearson相关系数在处理**非线性**关系时存在的问题，在处理存在特异点的情形时有优势，比如存在[特异点和高杠杆点](../特异点和高杠杆点)时。

#### 适用场景

- 单调关系（非常重要）
- 非线性关系
- 存在[特异点和高杠杆点](../特异点和高杠杆点)

### 肯德尔$\tau$相关系数（Kendall $\tau$ coefficient）

#### 定义

衡量两个定序变量的协同性，仍然是基于序号（秩）的。

#### 计算方法

对于n个样本点对：$(x_1,y_1),(x_2,y_2),...(x_n,y_n)$

$\forall{j>i; i,j = 1,2,...,n}$

$(x_i,y_i)$与$(x_j,y_j)$的秩方向相同：$(x_j-x_i)(y_j-y_i) > 0$
$(x_i,y_i)$与$(x_j,y_j)$的秩方向相反：$(x_j-x_i)(y_j-y_i) < 0$

全部数据所有可能的样本点对共有 

\[
\binom{n}{2} = \frac{n!}{2!(n-2)!} = \frac{n(n-1)}{2}
\]

用$N_c$表示同向数对的个数，用$N_d$表示反向数对的个数，则有

$$
\tau = \frac{N_c - N_d}{n(n-1)/2}
$$

#### 取值范围

从 -1 到 +1。

- +1: 完全一致的顺序
- -1: 完全不一致的顺序
- 0: 无一致性

#### 适用场景

适合于小样本或有较多重复值的数据，通常被认为比斯皮尔曼更稳健。比如多个评委对多个选手的打分场景，可以用于评判对一个选手的评判是否公平。

## 线性回归

所谓线性回归，就是要用**线性函数**来**拟合**数据，从而希望用来预测新样本数据的**预测**的过程。那么相应的，非线性回归用的就是非线性函数。

线性函数就是形如$y=b+ax$这样的函数，自变量和因变量之间是线性关系。

### 一元线性回归和多元线性回归

所谓一元就是一个自变量，多元就是多个自变量，一元是多元的一个特例，所以这里就放在一起说了。

对于总体数据，存在

$$
y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... + \beta_kx_{ik} + \epsilon_i
$$

- $y_i$是因变量Y在第$i$个样本点上的取值
- $\beta_0,\beta_1,...\beta_k$是未知的总体参数
- $x_{ij}$是自变量$X_j$在第$i$个样本点上的取值
- $\epsilon_i$是第$i$个随机误差项的取值

那么，回归建模要解决的问题就是求出$\beta_0,\beta_1,...\beta_k$。

## 求解方法

### 最小二乘法

关于最小二乘法的具体操作可查看[最小二乘法详解与拉格朗日求解过程](../最小二乘法详解与拉格朗日求解过程/)

### 模型评估方法

通过最小二乘法得到了回归函数的系数了，但这个函数真的能很好地解释自变量和因变量的变化规律吗？所以就要引出对模型质量的评估方法了。有如下几个指标

#### 拟合优度（Goodness of Fit）

判定系数是$R^2$（Coefficient of Determination）

$$
R^2 = \frac{\sum^n_{i=1}(\hat{y}-\bar{y})^2}{\sum^n_{i=1}(y_i-\bar{y})^2}
=\frac{用回归模型可解释的变异量}{总变异量}
$$

从定义来看，$R^2$越接近1，表示拟合优度越高，但实际并不是完全正相关的，需要具体问题具体分析。（有时过高的拟合优度可能代表[过度拟合](../过度拟合)）。

$R^2$的性质：

1. $0 <= R^2 <= 1$
2. 当$R^2=1$时，表示模型解释了所有的变异，说明模型的拟合效果非常好
3. 当$R^2=0$时，表示模型没有解释任何变异，说明模型的拟合效果很差
4. $r(x,y)=\sqrt{R^2}$，$r(x,y)$的符号取决于$b_1$的符号
5. 增加更多的自变量通常不会减少$R^2$的值，但即使这些变量与因变量无关，$R^2$也可能增加，这可能导致过度拟合的问题
6. $R^2$只适用于线性回归模型，对于非线性模型不一定有效。它不能用来判断模型是否合适，模型的残差分析同样重要

##### 调整后的测定系数

为了解决增加自变量导致的$R^2$的问题，通常使用调整后的$R^2$（$\bar{R}^2$）。

$$
\bar{R}^2 = 1 - (1-R^2) \cdot \frac{n-1}{n-p-1}
$$

其中，\( n \) 是样本大小，\( p \) 是自变量的数量。调整后的 \( R^2 \) 会惩罚增加不相关的自变量。

关于[F检验和t检验](../模型评估中的f检验和t检验)，但这篇文章只介绍了二者的定义，而没有介绍它在模型评估中的作用。

#### F-test

F-test用来检验回归模型的线性关系，即自变量和因变量之间是否存在线性关系。

举例：
$y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... + \beta_kx_{ik} + \epsilon_i$

F检验是用来验证$\forall_{i >= 1;} \beta_i \ne 0$

- 零假设 ($H_0$​)：所有自变量的系数都等于零，即模型没有解释力。
- 备择假设 ($H_1$​)：至少有一个自变量的系数不等于零，意味着模型有解释力。

所以对于一个多元线性模型，只需要做一次F检验。

计算F值后，与临界值进行比较，或使用p值进行判断。如果F值大于临界值或**p值小于显著性水平（通常为0.05）**，则拒绝零假设，表明模型具有统计显著性。

如果是非线性模型，F检验的结果可能不可靠。

#### t-test

t-test用来检验回归系数的显著性，即**某个**自变量对因变量是否有解释作用，所以有$n$个自变量的多元线性回归函数，就要进行$n$次t检验。同样是判断p值是否小于显著性水平。

> 显著性水平在Excel里是在P-value列，但在SPSS里是Sig.F

#### 交叉验证

泛化能力（Generalization Ability）是模型对新样本的预测能力。

前面提到了过度拟合，它指的是对训练样本的拟合能力，过度拟合带来的后果可能就是泛化能力不足，光顾着满足训练样本了，无法预测新样本。

那么，一个直观的想法就是把已有的数据集分成两部分，一部分作为训练集，用来建立模型，另一部分作为测试集，将测试集数据代入模型，验证模型的表现。

模型的预测精度可以用验证集的均方误差(PRESS, predicted Error Sum of Squares)来测量

$$
PRESS = \sum^{n_2}_{i=1}(y_i-\hat{y}_i)^2
$$

但是，很多实际情况下数据集并不足以拆分成两个足够大的训练集和测试集，这时候就要用到留一交叉验证法了。

简单来说就是依次从数据集中抽出一个作为测试集，用其他的作为训练集，第i个样本上的预测误差: $e^{\star}_i = y_i - \hat{y}_i$。那么模型的预测均方误差的估计值就是

$$
PRESS = \sum^n_{i=1}(e^{\star}_i)^2 = \sum^n_{i=1}(y_i-\hat{y}_i)^2
$$

### 变量筛选方法

接上面讲的t-test，有时我们会发现一些变量的P-value并不满足要求，即它对模型的解释性不显著，需要考虑把这种自变量剔除出模型，这就要用到变量筛选方法。

#### 向后筛选法(Backward Elimination)

这是最直接的方法，在用所有自变量生成的模型中，先把P-value大于0.05中的最大值对应的自变量去掉重新生成模型，如果还有大于0.05的就继续这一步操作，直到所有的P-value都小于0.05。

#### 向前选择法(Forward Selection)

反过来操作就是先计算因变量与每个自变量的一元回归模型，再从中找到P-value最小(且满足<0.05)的进入模型，然后在保留这个自变量的前提下再继续添加别的自变量。

#### 逐步回归法(Stepwise Regression)

但不管是向后筛选还是向前选择都有问题:

向后筛选法的问题是一旦一个自变量被剔除了，它就没机会再进入了，但随着其他自变量被删除，它的作用可能会显著起来。

向前选择法的问题是一旦一个自变量被加进来，它就不会再被剔出去了，但随着其他自变量的引入，一些先进入模型的变量的作用可能会变得不再显著。

总的来说就是没有考虑自变量之间的互相影响，这就需要逐步回归法来解决了。

方法：边进边退

起始：同前进法
结束：模型外的所有变量均不能通过t-检验

这个过程比较复杂，稍后用R语言的计算过程来演示。

## R语言实战

Excel无法实现变量筛选的自动化，SPSS用是能用，但是商用软件而且启动速度太慢了，所以这里用R语言来演示这个过程。

R Studio的安装非常简单，这里不再赘述，可以先看一篇非常优秀的文章：[Stepwise Regression in R](https://www.geeksforgeeks.org/stepwise-regression-in-r/)。

我们重点关注逐步回归法。

```
Start:  AIC=-33.22
Employed ~ GNP.deflator + GNP + Unemployed + Armed.Forces + Population + 
    Year

               Df Sum of Sq     RSS     AIC
- GNP.deflator  1   0.00292 0.83935 -35.163
- Population    1   0.00475 0.84117 -35.129
- GNP           1   0.10631 0.94273 -33.305
<none>                      0.83642 -33.219
- Year          1   1.49881 2.33524 -18.792
- Unemployed    1   1.59014 2.42656 -18.178
- Armed.Forces  1   2.16091 2.99733 -14.798

Step:  AIC=-35.16
Employed ~ GNP + Unemployed + Armed.Forces + Population + Year

               Df Sum of Sq    RSS     AIC
- Population    1   0.01933 0.8587 -36.799
<none>                      0.8393 -35.163
- GNP           1   0.14637 0.9857 -34.592
+ GNP.deflator  1   0.00292 0.8364 -33.219
- Year          1   1.52725 2.3666 -20.578
- Unemployed    1   2.18989 3.0292 -16.628
- Armed.Forces  1   2.39752 3.2369 -15.568

Step:  AIC=-36.8
Employed ~ GNP + Unemployed + Armed.Forces + Year

               Df Sum of Sq    RSS     AIC
<none>                      0.8587 -36.799
+ Population    1    0.0193 0.8393 -35.163
+ GNP.deflator  1    0.0175 0.8412 -35.129
- GNP           1    0.4647 1.3234 -31.879
- Year          1    1.8980 2.7567 -20.137
- Armed.Forces  1    2.3806 3.2393 -17.556
- Unemployed    1    4.0491 4.9077 -10.908
> 
> # Print the summary of the selected model
> summary(stepwise_model)

Call:
lm(formula = Employed ~ GNP + Unemployed + Armed.Forces + Year, 
    data = longley)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.42165 -0.12457 -0.02416  0.08369  0.45268 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -3.599e+03  7.406e+02  -4.859 0.000503 ***
GNP          -4.019e-02  1.647e-02  -2.440 0.032833 *  
Unemployed   -2.088e-02  2.900e-03  -7.202 1.75e-05 ***
Armed.Forces -1.015e-02  1.837e-03  -5.522 0.000180 ***
Year          1.887e+00  3.828e-01   4.931 0.000449 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.2794 on 11 degrees of freedom
Multiple R-squared:  0.9954,	Adjusted R-squared:  0.9937 
F-statistic: 589.8 on 4 and 11 DF,  p-value: 9.5e-13
```

可以看到，最终的模型带$\star$的都是满足要求的，把相关的系数代进去就是回归函数了。

那么，AIC是什么？

在R语言的逐步回归过程中，AIC（赤池信息量准则）和BIC（贝叶斯信息量准则）是两个常用的模型选择标准，用于评估和比较不同的统计模型。它们的主要作用是帮助选择合适的模型，同时考虑模型的复杂性和拟合优度。

### AIC（赤池信息量准则）

- **定义**：AIC是基于信息论的模型选择标准，其公式为：
  \[
  \text{AIC} = 2k - 2\ln(L)
  \]
  其中，\(k\)是模型中参数的数量，\(L\)是模型的似然函数。

- **作用**：AIC试图在拟合优度和模型复杂性之间取得平衡。较小的AIC值表示更优的模型。AIC倾向于选择较复杂的模型，因为它的惩罚力度相对较小。

### BIC（贝叶斯信息量准则）

- **定义**：BIC是基于贝叶斯理论的模型选择标准，其公式为：
  \[
  \text{BIC} = \ln(n)k - 2\ln(L)
  \]
  其中，\(n\)是样本数量，\(k\)是模型中参数的数量，\(L\)是模型的似然函数。

- **作用**：BIC也用于平衡模型的复杂性和拟合优度，但相较于AIC，它对模型复杂性的惩罚更为严格。BIC倾向于选择较简单的模型，尤其是在样本量较大时。

### AIC与BIC的比较

- **惩罚力度**：AIC的惩罚与参数数量成线性关系，而BIC的惩罚与样本量\(n\)相关，因此BIC对复杂模型的惩罚更大。
  
- **使用场景**：
  - **AIC**：在样本量较小或对模型复杂性不太敏感的情况下，AIC可能更适用。
  - **BIC**：在样本量较大时，BIC倾向于选择更简单的模型，适合于更关注模型的解释性的场景。

### 回到Julia

如果你看过我之前的文章可能会发现在学习运筹与决策时，我是更倾向于用Julia的，但这里我选择了R语言，这是因为在处理回归问题时，R语言比Julia优势明显，一个`step()`方法够Julia喝一壶的了，所以工具合适最重要，Julia的优势在于更加工程化、性能好，而R语言的优势在于包多，功能多。

## 自变量的多重相关性问题

前面提到了自变量之间的相互影响，这要怎么理解呢？

自变量的多重相关性（Multicollinearity）是指在回归分析中，自变量之间存在较强的线性关系。这种现象可能导致一些问题，影响模型的稳定性和解释能力。以下是关于多重相关性的一些重要点：

### 多重相关性的影响

- **不稳定的回归系数**：当自变量之间高度相关时，回归系数的估计可能变得不稳定，导致对自变量的影响不准确。
- **标准误差增大**：多重共线性会导致标准误差增大，从而影响t检验的结果，导致无法显著区分自变量的影响。
- **模型解释困难**：由于自变量之间的相互关系，难以确定哪个自变量真正对因变量有影响。

### 识别多重相关性

- **相关矩阵**：计算自变量之间的相关系数矩阵，查看哪些自变量之间的相关性较高。
- **方差膨胀因子（VIF）**：
    - VIF是一个常用的度量指标，用于检测多重相关性。VIF值越高，表示自变量的多重相关性越强。
    - 通常，VIF值大于10被认为有较强的多重共线性。
- **条件数**：条件数（Condition Number）也是检测多重相关性的一种方法，值越大，表示共线性越严重。

### 解决多重相关性的方法

- **删除变量**：根据相关性分析，删除一些自变量，以减少多重共线性。
- **合并变量**：将相关性强的变量合并为一个新的变量，例如通过主成分分析（PCA）。
- **正则化方法**：采用正则化回归（如Lasso回归、Ridge回归等），这些方法能够减小多重共线性带来的影响。
- **增加样本量**：在某些情况下，增加样本量可能会减轻多重共线性的问题。

### 注意事项

- 在某些情况下，多重相关性本身并不影响模型的预测能力，但仍然需要注意模型的解释性。
- 在建立模型时，理解自变量之间的关系，可以帮助研究人员做出更好的决策。

多重相关性是回归分析中常见的问题，会影响模型的稳定性和解释能力。通过识别和解决多重相关性，可以提高模型的可靠性和准确性。

## 非线性回归模型

上面主要讲了线性回归模型，但有些非线性回归模型可以通过某些变换转化为线性回归模型，从而利用线性回归方法进行分析。以下是一些常见的例子：

### 指数回归模型

- **非线性模型**：\( y = a e^{bx} \)
- **线性化**：取自然对数，两边取对数：
    \[
    \ln(y) = \ln(a) + bx
    \]
- **线性回归形式**：可以将其视为线性回归模型，其中 \( Y = \ln(y) \) 和 \( X = x \)。

### 幂律回归模型

- **非线性模型**：\( y = ax^b \)
- **线性化**：对两边取对数：
    \[
    \ln(y) = \ln(a) + b \ln(x)
    \]
- **线性回归形式**：可以将其视为线性回归模型，其中 \( Y = \ln(y) \) 和 \( X = \ln(x) \)。

### 逻辑回归模型

- **非线性模型**：\( P(y=1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}} \)
- **线性化**：通过逻辑斯蒂变换：
    \[
    \ln\left(\frac{P}{1-P}\right) = \beta_0 + \beta_1 x
    \]
- **线性回归形式**：将对数比视为因变量，使用线性回归进行分析。

### 二次回归模型

- **非线性模型**：\( y = ax^2 + bx + c \)
- **线性化**：可以重写为：
    \[
    y = c + b x + a x^2
    \]
- **线性回归形式**：将 \( x^2 \) 作为一个新的自变量，形成多元线性回归模型。

### 平方根回归

- **非线性模型**：\( y = a + b\sqrt{x} \)
- **线性化**：定义新的自变量 \( z = \sqrt{x} \)，则方程变为：
    \[
    y = a + bz
    \]
- **线性回归形式**：用 \( z \) 作为自变量进行线性回归。

通过对非线性模型进行适当的变换，可以将其转化为线性回归模型。这种方法使得我们能够利用线性回归的强大工具和技术来分析和预测数据。

## 总结

本文整体讲了线性回归模型中的一些概念和问题，包括相关系数、模型计算和检验、拟合优度、变量筛选（R语言为例）以及非线性回归模型转换成线性模型的例子，希望能对线性回归的学习有所帮助。
